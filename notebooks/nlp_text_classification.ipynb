{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c743983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras import preprocessing\n",
    "from keras import regularizers\n",
    "from keras import activations\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import models\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ccbe3",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21306dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/nn_data_1680619232.pickle'\n",
    "dictionary_path = '../data/dictionary_1680619234.pickle'\n",
    "\n",
    "maxlen=100\n",
    "max_words=10000\n",
    "first_split = 0.8\n",
    "second_split=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f2c7c",
   "metadata": {},
   "source": [
    "General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0207456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    if \"loss\" not in history.history:\n",
    "        print('Loss is missing in history')\n",
    "        return\n",
    "    \n",
    "    colors = ['b', 'r', 'g', 'y', 'w']\n",
    "    colors_dict = {}\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.title(\"History\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('metric values')\n",
    "    \n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    index = 0\n",
    "    for key_name, values in history.history.items():\n",
    "        if 'val_' in key_name:\n",
    "            plt.plot(epochs, values, colors_dict[key_name[4:]] + '.', label=key_name)\n",
    "        else:\n",
    "            colors_dict[key_name] = colors[index]\n",
    "            plt.plot(epochs, values, colors[index], label=key_name)\n",
    "            index += 1\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b5b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(model, x_test, y_test, batch_size):\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print(f\"Test loss: {round(loss, 4)}\")    \n",
    "    print(f\"Test acc: {round(acc, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de59abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, dictionary_path):\n",
    "    with open(data_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    with open(dictionary_path, 'rb') as file:\n",
    "        dictionary = pickle.load(file)\n",
    "        dictionary = {value: key for key, value in dictionary.items()}\n",
    "        \n",
    "    labels_count = len(dictionary)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for text, categories in data:\n",
    "        new_categories = np.zeros(labels_count, dtype='float32')\n",
    "#         for category in categories:\n",
    "#             new_categories[category] = 1.0\n",
    "        new_categories[categories[0]] = 1.0\n",
    "            \n",
    "        texts.append(text)\n",
    "        labels.append(new_categories)\n",
    "        \n",
    "        \n",
    "    return texts, labels, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b425aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(texts, labels, maxlen=None, max_words=10000):\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    if maxlen is not None:\n",
    "        data = pad_sequences(sequences, maxlen=maxlen)\n",
    "    else:\n",
    "        data = pad_sequences(sequences)\n",
    "\n",
    "    return data, np.asarray(labels), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46267364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, first_split=0.8, second_split=0.8):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=first_split)    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size=second_split)\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85c624",
   "metadata": {},
   "source": [
    "### Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e567b",
   "metadata": {},
   "source": [
    "Load preprocessed yelp dataset and dictionary. The dictionary is then inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c0c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, dictionary = load_data(data_path, dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f574b6d",
   "metadata": {},
   "source": [
    "Tokenize initially processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621919be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, tokenizer = prepare_data(texts, labels, maxlen=maxlen, max_words=max_words)\n",
    "texts = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374db4f8",
   "metadata": {},
   "source": [
    "Split the data to train, valid and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c30342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_data(data, labels, first_split=first_split, second_split=second_split)\n",
    "data = None\n",
    "labels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ebe7f",
   "metadata": {},
   "source": [
    "Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf26212",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/tokenizer_{int(time.time())}.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a81d6e",
   "metadata": {},
   "source": [
    "Print information about the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbf7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:   1537487\n",
      "Testing samples:    480465\n",
      "Validating samples: 384372\n"
     ]
    }
   ],
   "source": [
    "print(f'Training samples:   {x_train.shape[0]}')\n",
    "print(f'Testing samples:    {x_test.shape[0]}')\n",
    "print(f'Validating samples: {x_valid.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3550bf",
   "metadata": {},
   "source": [
    "### Create Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc00d5",
   "metadata": {},
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b683750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tc_model_2'\n",
    "embedding_dim = 25\n",
    "batch_size = 1024\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05369c2",
   "metadata": {},
   "source": [
    "Create model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6626e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 25)           250000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 100, 64)           11264     \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 33, 64)           0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 33, 64)            24960     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 63)                4158      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 63)                4032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 319,374\n",
      "Trainable params: 319,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    # embedding layer\n",
    "    layers.Embedding(max_words, embedding_dim, input_length=len(x_train[0])),\n",
    "    \n",
    "    # 1st convolutional layer\n",
    "    layers.Conv1D(64, 3, activation=layers.PReLU(), padding='same'),\n",
    "    layers.AveragePooling1D(3),\n",
    "    \n",
    "    # GRU layers\n",
    "    layers.GRU(64, return_sequences=True),\n",
    "    layers.GRU(64),\n",
    "#     , kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=5e-4)\n",
    "    \n",
    "    # flatten\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # 1st dense layer\n",
    "    layers.Dense(len(dictionary), activation=layers.PReLU()),\n",
    "    \n",
    "    # last layer\n",
    "    layers.Dense(len(dictionary), activation='softplus')\n",
    "])\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss=losses.CategoricalCrossentropy(),\n",
    "    optimizer=optimizers.Nadam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcc9a8",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40612b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1502/1502 [==============================] - 65s 40ms/step - loss: 3.0694 - acc: 0.1781 - val_loss: 2.8194 - val_acc: 0.2416\n",
      "Epoch 2/10\n",
      "1502/1502 [==============================] - 59s 39ms/step - loss: 2.6911 - acc: 0.2728 - val_loss: 2.6172 - val_acc: 0.2916\n",
      "Epoch 3/10\n",
      "1502/1502 [==============================] - 59s 39ms/step - loss: 2.5649 - acc: 0.3045 - val_loss: 2.5373 - val_acc: 0.3126\n",
      "Epoch 4/10\n",
      "1502/1502 [==============================] - 59s 39ms/step - loss: 2.5057 - acc: 0.3186 - val_loss: 2.5017 - val_acc: 0.3210\n",
      "Epoch 5/10\n",
      "1502/1502 [==============================] - 59s 39ms/step - loss: 2.4682 - acc: 0.3269 - val_loss: 2.4745 - val_acc: 0.3263\n",
      "Epoch 6/10\n",
      "1502/1502 [==============================] - 59s 39ms/step - loss: 2.4402 - acc: 0.3325 - val_loss: 2.4588 - val_acc: 0.3292\n",
      "Epoch 7/10\n",
      "1502/1502 [==============================] - 59s 39ms/step - loss: 2.4178 - acc: 0.3373 - val_loss: 2.4428 - val_acc: 0.3331\n",
      "Epoch 8/10\n",
      "1023/1502 [===================>..........] - ETA: 17s - loss: 2.3980 - acc: 0.3414"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Could not synchronize CUDA stream: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mInternalError\u001b[0m: Could not synchronize CUDA stream: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86fb64",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'../models/{model_name}_{int(time.time())}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b80fa",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(model, x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd625f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15_2 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
