{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c743983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras import preprocessing\n",
    "from keras import regularizers\n",
    "from keras import activations\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import models\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ccbe3",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21306dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/nn_data_1680619232.pickle'\n",
    "dictionary_path = '../data/dictionary_1680619234.pickle'\n",
    "\n",
    "maxlen=200\n",
    "max_words=10000\n",
    "first_split = 0.8\n",
    "second_split=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f2c7c",
   "metadata": {},
   "source": [
    "General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0207456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    epochs = range(1, len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(model, x_test, y_test, labels):\n",
    "    # todo\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(model, x_test, y_test, batch_size):\n",
    "    loss = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print(f\"Test loss: {round(loss, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de59abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, dictionary_path):\n",
    "    with open(data_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    with open(dictionary_path, 'rb') as file:\n",
    "        dictionary = pickle.load(file)\n",
    "        dictionary = {value: key for key, value in dictionary.items()}\n",
    "        \n",
    "    labels_count = len(dictionary)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for text, categories in data:\n",
    "        new_categories = np.zeros(labels_count, dtype='float32')\n",
    "        for category in categories:\n",
    "            new_categories[category] = 1.0\n",
    "            \n",
    "        texts.append(text)\n",
    "        labels.append(new_categories)\n",
    "        \n",
    "        \n",
    "    return texts, labels, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b425aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(texts, labels, maxlen=None, max_words=10000):\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    if maxlen is not None:\n",
    "        data = pad_sequences(sequences, maxlen=maxlen)\n",
    "    else:\n",
    "        data = pad_sequences(sequences)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46267364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, first_split=0.8, second_split=0.8):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=first_split)    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size=second_split)\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85c624",
   "metadata": {},
   "source": [
    "### Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e567b",
   "metadata": {},
   "source": [
    "Load preprocessed yelp dataset and dictionary. The dictionary is then inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c0c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, dictionary = load_data(data_path, dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f574b6d",
   "metadata": {},
   "source": [
    "Tokenize initially processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621919be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = prepare_data(texts, labels, maxlen=maxlen, max_words=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374db4f8",
   "metadata": {},
   "source": [
    "Split the data to train, valid and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c30342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_data(data, labels, first_split=first_split, second_split=second_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a81d6e",
   "metadata": {},
   "source": [
    "Print information about the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cbf7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:   1537487\n",
      "Testing samples:    480465\n",
      "Validating samples: 384372\n"
     ]
    }
   ],
   "source": [
    "print(f'Training samples:   {x_train.shape[0]}')\n",
    "print(f'Testing samples:    {x_test.shape[0]}')\n",
    "print(f'Validating samples: {x_valid.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3550bf",
   "metadata": {},
   "source": [
    "### Create Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc00d5",
   "metadata": {},
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b683750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tc_model_1'\n",
    "embedding_dim = 25\n",
    "batch_size = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05369c2",
   "metadata": {},
   "source": [
    "Create model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6626e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:11:07.182375: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-04 18:11:07.182521: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 25)           250000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 200, 64)           17664     \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 66, 64)           0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 66, 64)            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 66, 64)            16576     \n",
      "                                                                 \n",
      " average_pooling1d_1 (Averag  (None, 22, 64)           0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 22, 64)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 22, 64)            13760     \n",
      "                                                                 \n",
      " average_pooling1d_2 (Averag  (None, 7, 64)            0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7, 64)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 448)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 48)                21600     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 48)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 63)                3087      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322,687\n",
      "Trainable params: 322,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    # embedding layer\n",
    "    layers.Embedding(max_words, embedding_dim, input_length=len(x_train[0])),\n",
    "    \n",
    "    # 1st convolutional layer\n",
    "    layers.Conv1D(64, 3, activation=layers.PReLU(), padding='same', kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=5e-4)),\n",
    "    layers.AveragePooling1D(3),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # 2nd convolutional layer\n",
    "    layers.Conv1D(64, 3, activation=layers.PReLU(), padding='same', kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=5e-4)),\n",
    "    layers.AveragePooling1D(3),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # 3rd convolutional layer\n",
    "    layers.Conv1D(64, 3, activation=layers.PReLU(), padding='same', kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=5e-4)),\n",
    "    layers.AveragePooling1D(3),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # flatten\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # 1st dense layer\n",
    "    layers.Dense(48, activation=layers.PReLU(), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=5e-4)),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    # last layer\n",
    "    layers.Dense(len(dictionary), activation='sigmoid', kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=5e-4))\n",
    "])\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss='mae',\n",
    "    optimizer=optimizers.Nadam()\n",
    ")\n",
    "\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcc9a8",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40612b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86fb64",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'../data/{model_name}_{int(time.time())}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b80fa",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(model, x_test, y_test, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
