{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4edcba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import keras\n",
    "import time\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f99ae",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8b8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/tc_model_2_1681463923_tt_1681463733.h5'\n",
    "tokenizer_path = '../models/tokenizer_1681463733.pickle'\n",
    "detailed_data_path = '../data/places_krakow_detailed_1680895219.pickle'\n",
    "city_name = 'krakow'\n",
    "creation_time = int(time.time())\n",
    "maxlen = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60c70a",
   "metadata": {},
   "source": [
    "The layers for transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ade2ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, ff_act='relu', ff_reg=None, ff_d=0.25, mh_reg=None, mh_d=0.1, norm_eps=1e-6, **kwargs):\n",
    "        # initialize super class\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        \n",
    "        # multi head attention\n",
    "        self.att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim,\n",
    "            kernel_regularizer=mh_reg,\n",
    "            dropout=mh_d\n",
    "        )\n",
    "        \n",
    "        # feed forward network\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=ff_act, kernel_regularizer=ff_reg), \n",
    "            layers.Dense(embed_dim, kernel_regularizer=ff_reg)\n",
    "        ])\n",
    "        \n",
    "        # layer normalizations\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=norm_eps)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=norm_eps)\n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout1 = layers.Dropout(ff_d)\n",
    "        self.dropout2 = layers.Dropout(ff_d)\n",
    "        \n",
    "        # remember for serialization\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.ff_act=ff_act\n",
    "        self.ff_reg=ff_reg\n",
    "        self.ff_d=ff_d\n",
    "        self.mh_reg=mh_reg\n",
    "        self.mh_d=mh_d\n",
    "        self.norm_eps=norm_eps\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"ff_act\": self.ff_act,\n",
    "            \"ff_reg\": self.ff_reg,\n",
    "            \"ff_d\": self.ff_d,\n",
    "            \"mh_reg\": self.mh_reg,\n",
    "            \"mh_d\": self.mh_d,\n",
    "            \"norm_eps\": self.norm_eps\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4fa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, embed_reg=None, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        \n",
    "        # embedding layers\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embed_dim,\n",
    "            embeddings_regularizer=embed_reg\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(\n",
    "            input_dim=maxlen, \n",
    "            output_dim=embed_dim,\n",
    "            embeddings_regularizer=embed_reg\n",
    "        )\n",
    "        \n",
    "        # save for serialization\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_reg = embed_reg\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"embed_reg\": self.embed_reg\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b1a1f",
   "metadata": {},
   "source": [
    "### Add the vector column to detailed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06551c8",
   "metadata": {},
   "source": [
    "The method loads neural network model, tokenizer and the detailed places data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f8e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_path, tokenizer_path, detailed_data_path):\n",
    "    with open(tokenizer_path, 'rb') as file:\n",
    "        tokenizer = pickle.load(file)\n",
    "    with open(detailed_data_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    model = keras.models.load_model(\n",
    "        filepath = model_path, \n",
    "        custom_objects = {\n",
    "            \"TokenAndPositionEmbedding\": TokenAndPositionEmbedding,\n",
    "            \"TransformerBlock\": TransformerBlock\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33e6a2",
   "metadata": {},
   "source": [
    "The methods are responsible for describing each place (the default method concatenates its types, summary and reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546b231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_describer(place):\n",
    "    description = ''\n",
    "    for place_type in place[2]:\n",
    "        description += (place_type.replace('_', ' ')) + ' '\n",
    "\n",
    "    if place[8] is not None:\n",
    "        description += place[8] + ' '\n",
    "\n",
    "    for review in place[9]:\n",
    "        description += review.replace('\\n', '').replace('\\\\', '') + ' '\n",
    "\n",
    "    return description.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b94b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_reviews_describer(place):\n",
    "    description = ''\n",
    "    if place[8] is not None:\n",
    "        description += place[8] + ' '\n",
    "\n",
    "    for review in place[9]:\n",
    "        description += review.replace('\\n', '').replace('\\\\', '') + ' '\n",
    "\n",
    "    return description.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a26e4",
   "metadata": {},
   "source": [
    "The method adds additional column that describes places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae584d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(data, model, tokenizer, describer=default_describer, description_length_threshold=400):\n",
    "    result = []\n",
    "    for place in data:\n",
    "        description = describer(place)\n",
    "        if len(description) >= description_length_threshold:\n",
    "            sequences = tokenizer.texts_to_sequences([description])\n",
    "            x = pad_sequences(sequences, maxlen=maxlen)\n",
    "            vector = model.predict(x, verbose=0)[0]\n",
    "            place.append(vector)\n",
    "            result.append(place)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52295cd0",
   "metadata": {},
   "source": [
    "Use created methods to load model, tokenizer and information about places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "809c4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, data = load(model_path, tokenizer_path, detailed_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f3c94",
   "metadata": {},
   "source": [
    "Vectorize loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea62e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized = vectorize_data(data, model, tokenizer, describer=summary_reviews_describer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333be93",
   "metadata": {},
   "source": [
    "Save the vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1eebe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/places_{city_name}_vectorized_{creation_time}.pickle', 'wb') as file:\n",
    "    pickle.dump(data_vectorized, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c70df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15_2 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
