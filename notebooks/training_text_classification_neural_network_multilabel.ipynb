{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c743983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras import preprocessing\n",
    "from keras import regularizers\n",
    "from keras import activations\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import models\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ccbe3",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21306dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/large_nn_data_1681467705.pickle'\n",
    "dictionary_path = '../data/large_dictionary_1681467705.pickle'\n",
    "tokenizer_path = '../models/tokenizer_1681467787.pickle'  # if none the new one is created\n",
    "\n",
    "maxlen=500\n",
    "max_words=20000\n",
    "first_split = 0.9\n",
    "second_split=0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f2c7c",
   "metadata": {},
   "source": [
    "General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0207456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    epochs = range(1, len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b5b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(model, x_test, y_test, batch_size):\n",
    "    loss = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print(f\"Test loss: {round(loss, 4)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de59abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, dictionary_path):\n",
    "    with open(data_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    with open(dictionary_path, 'rb') as file:\n",
    "        dictionary = pickle.load(file)\n",
    "        dictionary = {value: key for key, value in dictionary.items()}\n",
    "        \n",
    "    labels_count = len(dictionary)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for text, category in data:\n",
    "        new_categories = np.zeros(labels_count, dtype='float32')\n",
    "        new_categories[category] = 1.0\n",
    "        texts.append(text)\n",
    "        labels.append(new_categories)\n",
    "        \n",
    "        \n",
    "    return texts, labels, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b425aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(texts, labels, t_path=None, maxlen=None, max_words=10000):\n",
    "    if t_path is None:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "    else:\n",
    "        with open(t_path, 'rb') as file:\n",
    "            tokenizer = pickle.load(file)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    if maxlen is not None:\n",
    "        data = pad_sequences(sequences, maxlen=maxlen)\n",
    "    else:\n",
    "        data = pad_sequences(sequences)\n",
    "\n",
    "    return data, np.asarray(labels), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46267364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, first_split=0.8, second_split=0.8):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=first_split)    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size=second_split)\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85c624",
   "metadata": {},
   "source": [
    "### Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e567b",
   "metadata": {},
   "source": [
    "Load preprocessed yelp dataset and dictionary. The dictionary is then inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c0c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, dictionary = load_data(data_path, dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f574b6d",
   "metadata": {},
   "source": [
    "Tokenize initially processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621919be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, tokenizer = prepare_data(texts, labels, tokenizer_path, maxlen=maxlen, max_words=max_words)\n",
    "texts = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374db4f8",
   "metadata": {},
   "source": [
    "Split the data to train, valid and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c30342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_data(data, labels, first_split=first_split, second_split=second_split)\n",
    "data = None\n",
    "labels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ebe7f",
   "metadata": {},
   "source": [
    "Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf26212",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer_path is None:\n",
    "    tokenizer_time = int(time.time())\n",
    "    with open(f'../models/tokenizer_{tokenizer_time}.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    tokenizer_time = int(tokenizer_path.split('tokenizer_')[-1][0:-7])\n",
    "tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a81d6e",
   "metadata": {},
   "source": [
    "Print information about the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbf7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:   240823\n",
      "Testing samples:    29732\n",
      "Validating samples: 26759\n"
     ]
    }
   ],
   "source": [
    "print(f'Training samples:   {x_train.shape[0]}')\n",
    "print(f'Testing samples:    {x_test.shape[0]}')\n",
    "print(f'Validating samples: {x_valid.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424e38a",
   "metadata": {},
   "source": [
    "### Train multi-label model v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5eb2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, ff_act='relu', ff_reg=None, ff_d=0.25, mh_reg=None, mh_d=0.1, norm_eps=1e-6, **kwargs):\n",
    "        # initialize super class\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        \n",
    "        # multi head attention\n",
    "        self.att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim,\n",
    "            kernel_regularizer=mh_reg,\n",
    "            dropout=mh_d\n",
    "        )\n",
    "        \n",
    "        # feed forward network\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=ff_act, kernel_regularizer=ff_reg), \n",
    "            layers.Dense(embed_dim, kernel_regularizer=ff_reg)\n",
    "        ])\n",
    "        \n",
    "        # layer normalizations\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=norm_eps)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=norm_eps)\n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout1 = layers.Dropout(ff_d)\n",
    "        self.dropout2 = layers.Dropout(ff_d)\n",
    "        \n",
    "        # remember for serialization\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.ff_act=ff_act\n",
    "        self.ff_reg=ff_reg\n",
    "        self.ff_d=ff_d\n",
    "        self.mh_reg=mh_reg\n",
    "        self.mh_d=mh_d\n",
    "        self.norm_eps=norm_eps\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"ff_act\": self.ff_act,\n",
    "            \"ff_reg\": self.ff_reg,\n",
    "            \"ff_d\": self.ff_d,\n",
    "            \"mh_reg\": self.mh_reg,\n",
    "            \"mh_d\": self.mh_d,\n",
    "            \"norm_eps\": self.norm_eps\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd79a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, embed_reg=None, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        \n",
    "        # embedding layers\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embed_dim,\n",
    "            embeddings_regularizer=embed_reg\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(\n",
    "            input_dim=maxlen, \n",
    "            output_dim=embed_dim,\n",
    "            embeddings_regularizer=embed_reg\n",
    "        )\n",
    "        \n",
    "        # save for serialization\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_reg = embed_reg\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"embed_reg\": self.embed_reg\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df354ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tc_ml_model_1'\n",
    "embedding_dim = 32\n",
    "number_heads = 4\n",
    "feed_forward_dim = 32\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df7d2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 500, 32)          656000    \n",
      " g_2 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  (None, 500, 32)          19040     \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 32)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " layer_normalization_8 (Laye  (None, 32)               64        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 62)                2046      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 677,150\n",
      "Trainable params: 677,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input layer\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "\n",
    "# position encoding & embedding\n",
    "embedding_layer = TokenAndPositionEmbedding(\n",
    "    maxlen = maxlen, \n",
    "    vocab_size = max_words, \n",
    "    embed_dim = embedding_dim, \n",
    "    embed_reg = None\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# multi head attention & feed forward\n",
    "transformer_block = TransformerBlock(\n",
    "    embed_dim = embedding_dim, \n",
    "    num_heads = number_heads, \n",
    "    ff_dim = feed_forward_dim, \n",
    "    ff_act = 'relu', \n",
    "    ff_reg = None,\n",
    "    ff_d = 0.25, \n",
    "    mh_reg = None, \n",
    "    mh_d = 0.25, \n",
    "    norm_eps = 1e-3\n",
    ")\n",
    "x = transformer_block(x)\n",
    "\n",
    "# # global average pooling & normalization\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.LayerNormalization()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "\n",
    "# last layer\n",
    "outputs = layers.Dense(len(dictionary), activation='sigmoid', kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4))(x)\n",
    "\n",
    "# creating model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=optimizers.Nadam(learning_rate=0.001, clipvalue=1.0, clipnorm=1.0),\n",
    ")\n",
    "\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca71be73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1882/1882 [==============================] - 408s 215ms/step - loss: 0.0339 - val_loss: 0.0203\n",
      "Epoch 2/5\n",
      "1882/1882 [==============================] - 441s 234ms/step - loss: 0.0200 - val_loss: 0.0183\n",
      "Epoch 3/5\n",
      " 967/1882 [==============>...............] - ETA: 3:21 - loss: 0.0186"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model_2/transformer_block_2/multi_head_attention_2/einsum_1/Einsum' defined at (most recent call last):\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Kamil\\AppData\\Local\\Temp\\ipykernel_18296\\1383914482.py\", line 1, in <module>\n      history = model.fit(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model_2/transformer_block_2/multi_head_attention_2/einsum_1/Einsum'\nOOM when allocating tensor with shape[128,4,500,500] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_2/transformer_block_2/multi_head_attention_2/einsum_1/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_13754]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_tt_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_time\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{epoch:02d}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{val_loss:.4f}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_weights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_value_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model_2/transformer_block_2/multi_head_attention_2/einsum_1/Einsum' defined at (most recent call last):\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Kamil\\AppData\\Local\\Temp\\ipykernel_18296\\1383914482.py\", line 1, in <module>\n      history = model.fit(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model_2/transformer_block_2/multi_head_attention_2/einsum_1/Einsum'\nOOM when allocating tensor with shape[128,4,500,500] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_2/transformer_block_2/multi_head_attention_2/einsum_1/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_13754]"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=epochs,  \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[\n",
    "        callbacks.ModelCheckpoint('../models/'+model_name+'_'+str(int(time.time()))+'_tt_'+str(tokenizer_time)+'_{epoch:02d}_{val_loss:.4f}.h5', monitor='val_loss', verbose=0, save_weights_only=False, save_best_only=True, mode='min', initial_value_threshold=0.05)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58b66fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'../models/{model_name}_{int(time.time())}_tt_{tokenizer_time}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b76d3",
   "metadata": {},
   "source": [
    "### Visualize model v1 training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c91e05d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_history(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f01f6a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "print_evaluation(model, x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7aa48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15_2 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
